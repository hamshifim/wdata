{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Bee Hive data https://drive.google.com/file/d/142IBcs6OyQiJxO7owPfkEBFbkrudnh0g/view?usp=sharing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "APP = 'BeeHive'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "''..' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "# ! '../../package_py.bash'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType\n",
    "from pyspark.sql.functions import split, row_number, udf, col, min\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import random\n",
    "#from treelib import Node, Tree\n",
    "from treelib import Tree\n",
    "\n",
    "from pep_data.project import quick_conf\n",
    "from src.pep_data.spark.util import base_df\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1809e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(APP).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Get app configuration from project.conf file\n",
    "conf = quick_conf()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f950c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+\n",
      "|Bee ID|DaughtersEfficiencyScore|Father SIZE|Father TYPE|           X|          Y|          Z|\n",
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+\n",
      "|   0_0|           -0.1647494899|          5|     107027|    5.345708|25.54008605|24.79858692|\n",
      "|   0_9|          -0.09111780618|          8|      35473|  3.61617713|15.39993678|14.11150683|\n",
      "|  0_16|           -0.2783737487|          9|      72732| 6.487132473|27.96111467|23.50405554|\n",
      "|  0_76|           0.01988315069|          6|      49069| 8.285176906|21.88111447|10.80561155|\n",
      "|  0_35|           -0.0758420403|          5|        187|        -1.0|       13.0|       17.0|\n",
      "|  0_17|           -0.3362110457|          5|      74276|        -0.6|       12.0|       16.0|\n",
      "|  0_49|           -0.1311013747|         10|     108253| 4.123056465|16.28346035| 12.7359525|\n",
      "|  0_11|           -0.1881197044|          5|       8907|         2.4|       14.9|       12.7|\n",
      "|  0_53|             -1.07835423|          5|      56276|  0.48934967|24.60089192|24.29571403|\n",
      "|  0_58|           -0.1457884704|          5|       8639|        4.75|      15.75|      14.25|\n",
      "|  0_29|           -0.3677328995|          9|      16678| 5.347068171|24.86318585| 13.1676809|\n",
      "|  0_36|            -0.276260889|          7|      71010|        0.35|       15.0|      16.65|\n",
      "|   0_1|           -0.0337883593|         13|      23028|        0.85|      23.85|      22.25|\n",
      "|  0_60|           -0.7111252834|          7|      73856|0.6689851446|25.63599441|24.23511828|\n",
      "|   0_3|          -0.04991202534|         14|        782|0.8929233119|18.07738731| 16.2277547|\n",
      "|   0_4|           -0.2344739595|         12|      45682|  3.45375924|27.39807645|17.41006477|\n",
      "|  0_54|           -0.3550285031|          5|      35576|        -0.6|       22.9|       22.4|\n",
      "|  0_41|          -0.07477852786|          9|      88641|         1.5|       24.5|       21.6|\n",
      "|   0_5|         -0.004817857963|         14|        191| 3.198876948|24.93546125|15.27157968|\n",
      "|  0_48|            -0.237260917|          6|       1632| 9.395030718|14.13030347| 10.2961167|\n",
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = base_df(spark, conf, APP)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248b2a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+-----+--------+\n",
      "|Bee ID|DaughtersEfficiencyScore|Father SIZE|Father TYPE|           X|          Y|          Z|Cycle|Cycle ID|\n",
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+-----+--------+\n",
      "|   0_0|           -0.1647494899|          5|     107027|    5.345708|25.54008605|24.79858692|    0|       0|\n",
      "|   0_9|          -0.09111780618|          8|      35473|  3.61617713|15.39993678|14.11150683|    0|       9|\n",
      "|  0_16|           -0.2783737487|          9|      72732| 6.487132473|27.96111467|23.50405554|    0|      16|\n",
      "|  0_76|           0.01988315069|          6|      49069| 8.285176906|21.88111447|10.80561155|    0|      76|\n",
      "|  0_35|           -0.0758420403|          5|        187|        -1.0|       13.0|       17.0|    0|      35|\n",
      "|  0_17|           -0.3362110457|          5|      74276|        -0.6|       12.0|       16.0|    0|      17|\n",
      "|  0_49|           -0.1311013747|         10|     108253| 4.123056465|16.28346035| 12.7359525|    0|      49|\n",
      "|  0_11|           -0.1881197044|          5|       8907|         2.4|       14.9|       12.7|    0|      11|\n",
      "|  0_53|             -1.07835423|          5|      56276|  0.48934967|24.60089192|24.29571403|    0|      53|\n",
      "|  0_58|           -0.1457884704|          5|       8639|        4.75|      15.75|      14.25|    0|      58|\n",
      "|  0_29|           -0.3677328995|          9|      16678| 5.347068171|24.86318585| 13.1676809|    0|      29|\n",
      "|  0_36|            -0.276260889|          7|      71010|        0.35|       15.0|      16.65|    0|      36|\n",
      "|   0_1|           -0.0337883593|         13|      23028|        0.85|      23.85|      22.25|    0|       1|\n",
      "|  0_60|           -0.7111252834|          7|      73856|0.6689851446|25.63599441|24.23511828|    0|      60|\n",
      "|   0_3|          -0.04991202534|         14|        782|0.8929233119|18.07738731| 16.2277547|    0|       3|\n",
      "|   0_4|           -0.2344739595|         12|      45682|  3.45375924|27.39807645|17.41006477|    0|       4|\n",
      "|  0_54|           -0.3550285031|          5|      35576|        -0.6|       22.9|       22.4|    0|      54|\n",
      "|  0_41|          -0.07477852786|          9|      88641|         1.5|       24.5|       21.6|    0|      41|\n",
      "|   0_5|         -0.004817857963|         14|        191| 3.198876948|24.93546125|15.27157968|    0|       5|\n",
      "|  0_48|            -0.237260917|          6|       1632| 9.395030718|14.13030347| 10.2961167|    0|      48|\n",
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split Bee_ID column to Cycle(Bee value) and Cycle ID(ID value) columns\n",
    "df_cleaned = df.withColumn('Cycle', split(col('Bee ID'), '_')\\\n",
    "                           .getItem(0))\\\n",
    "               .withColumn('Cycle ID', split(col('Bee ID'), '_')\\\n",
    "                           .getItem(1))\n",
    "\n",
    "# Change the type of value in Cycle column from string to integer\n",
    "df_cleaned= df_cleaned.withColumn(\"Cycle\",col(\"Cycle\")\\\n",
    "                                  .cast(IntegerType()))\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9598022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+-----+--------+-------------+\n",
      "|Bee ID|DaughtersEfficiencyScore|Father SIZE|Father TYPE|           X|          Y|          Z|Cycle|Cycle ID|Continuous ID|\n",
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+-----+--------+-------------+\n",
      "|   0_0|           -0.1647494899|          5|     107027|    5.345708|25.54008605|24.79858692|    0|       0|            1|\n",
      "|   0_9|          -0.09111780618|          8|      35473|  3.61617713|15.39993678|14.11150683|    0|       9|            2|\n",
      "|  0_16|           -0.2783737487|          9|      72732| 6.487132473|27.96111467|23.50405554|    0|      16|            3|\n",
      "|  0_76|           0.01988315069|          6|      49069| 8.285176906|21.88111447|10.80561155|    0|      76|            4|\n",
      "|  0_35|           -0.0758420403|          5|        187|        -1.0|       13.0|       17.0|    0|      35|            5|\n",
      "|  0_17|           -0.3362110457|          5|      74276|        -0.6|       12.0|       16.0|    0|      17|            6|\n",
      "|  0_49|           -0.1311013747|         10|     108253| 4.123056465|16.28346035| 12.7359525|    0|      49|            7|\n",
      "|  0_11|           -0.1881197044|          5|       8907|         2.4|       14.9|       12.7|    0|      11|            8|\n",
      "|  0_53|             -1.07835423|          5|      56276|  0.48934967|24.60089192|24.29571403|    0|      53|            9|\n",
      "|  0_58|           -0.1457884704|          5|       8639|        4.75|      15.75|      14.25|    0|      58|           10|\n",
      "|  0_29|           -0.3677328995|          9|      16678| 5.347068171|24.86318585| 13.1676809|    0|      29|           11|\n",
      "|  0_36|            -0.276260889|          7|      71010|        0.35|       15.0|      16.65|    0|      36|           12|\n",
      "|   0_1|           -0.0337883593|         13|      23028|        0.85|      23.85|      22.25|    0|       1|           13|\n",
      "|  0_60|           -0.7111252834|          7|      73856|0.6689851446|25.63599441|24.23511828|    0|      60|           14|\n",
      "|   0_3|          -0.04991202534|         14|        782|0.8929233119|18.07738731| 16.2277547|    0|       3|           15|\n",
      "|   0_4|           -0.2344739595|         12|      45682|  3.45375924|27.39807645|17.41006477|    0|       4|           16|\n",
      "|  0_54|           -0.3550285031|          5|      35576|        -0.6|       22.9|       22.4|    0|      54|           17|\n",
      "|  0_41|          -0.07477852786|          9|      88641|         1.5|       24.5|       21.6|    0|      41|           18|\n",
      "|   0_5|         -0.004817857963|         14|        191| 3.198876948|24.93546125|15.27157968|    0|       5|           19|\n",
      "|  0_48|            -0.237260917|          6|       1632| 9.395030718|14.13030347| 10.2961167|    0|      48|           20|\n",
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+-----+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the data frame by Cycle column and add row number for each row(new column with the name Continuous ID)\n",
    "w = Window().orderBy('Cycle')\n",
    "df_cleaned = df_cleaned.withColumn('Continuous ID', row_number()\\\n",
    "                                   .over(w))\n",
    "\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40b390d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 1,\n 1: 551,\n 2: 1859,\n 3: 3224,\n 4: 4453,\n 5: 5171,\n 6: 5840,\n 7: 6541,\n 8: 7394,\n 9: 8071,\n 10: 8782,\n 11: 9290,\n 12: 10863,\n 13: 12233,\n 14: 13375,\n 15: 14053,\n 16: 14766,\n 17: 15458,\n 18: 16203,\n 19: 16915,\n 20: 17700,\n 21: 18323,\n 22: 19045,\n 23: 19859,\n 24: 20535,\n 25: 21226,\n 26: 21862,\n 27: 22595,\n 28: 23384,\n 29: 24147,\n 30: 24944,\n 31: 25730,\n 32: 26403,\n 33: 27153,\n 34: 27796,\n 35: 28645,\n 36: 29505,\n 37: 30313,\n 38: 31053,\n 39: 31688,\n 40: 32331,\n 41: 33170,\n 42: 34095,\n 43: 34753,\n 44: 35502,\n 45: 36433,\n 46: 37375,\n 47: 38147,\n 48: 38878,\n 49: 39590,\n 50: 40388,\n 51: 41283,\n 52: 43136,\n 53: 44282,\n 54: 45148,\n 55: 46070,\n 56: 46859,\n 57: 47902,\n 58: 48614,\n 59: 49444,\n 60: 50387,\n 61: 51066,\n 62: 51920,\n 63: 52674,\n 64: 53467,\n 65: 54124,\n 66: 54863,\n 67: 55537,\n 68: 56387,\n 69: 57250,\n 70: 58143,\n 71: 58892,\n 72: 59758,\n 73: 60472,\n 74: 61223,\n 75: 61953,\n 76: 62685,\n 77: 63550,\n 78: 64324,\n 79: 65146,\n 80: 65767,\n 81: 66350,\n 82: 67220,\n 83: 67951,\n 84: 68655,\n 85: 69325,\n 86: 69956,\n 87: 70677,\n 88: 71305,\n 89: 72146,\n 90: 72890,\n 91: 73595,\n 92: 74289,\n 93: 75191,\n 94: 76718,\n 95: 77810,\n 96: 78731,\n 97: 79437,\n 98: 80135,\n 99: 80491}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary with key = cycle , value = minimum value of Continuous ID of cycle(key)\n",
    "continuous_min_id_per_cycle = {key : value for key, value  in df_cleaned.groupBy('Cycle').min('Continuous ID').collect()}\n",
    "\n",
    "continuous_min_id_per_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5502f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[0,\n 1,\n 2,\n 3,\n 4,\n 5,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 13,\n 14,\n 15,\n 16,\n 17,\n 18,\n 19,\n 20,\n 21,\n 22,\n 23,\n 24,\n 25,\n 26,\n 27,\n 28,\n 29,\n 30,\n 31,\n 32,\n 33,\n 34,\n 35,\n 36,\n 37,\n 38,\n 39,\n 40,\n 41,\n 42,\n 43,\n 44,\n 45,\n 46,\n 47,\n 48,\n 49,\n 50,\n 51,\n 52,\n 53,\n 54,\n 55,\n 56,\n 57,\n 58,\n 59,\n 60,\n 61,\n 62,\n 63,\n 64,\n 65,\n 66,\n 67,\n 68,\n 69,\n 70,\n 71,\n 72,\n 73,\n 74,\n 75,\n 76,\n 77,\n 78,\n 79,\n 80,\n 81,\n 82,\n 83,\n 84,\n 85,\n 86,\n 87,\n 88,\n 89,\n 90,\n 91,\n 92,\n 93,\n 94,\n 95,\n 96,\n 97,\n 98,\n 99]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sorted list of all distinct values of Cycle column\n",
    "cycles = sorted([i[0] for i in df_cleaned.select('Cycle').distinct().collect()])\n",
    "\n",
    "cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80aa79ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return parent continuous id according to cycle and n\n",
    "def assert_parent_bee_id(cycle):\n",
    "    n = 3\n",
    "\n",
    "    # Get index  of cycle in cycles list\n",
    "    cycle_index = cycles.index(cycle)\n",
    "\n",
    "    # Return parent bee id if cycle is 0\n",
    "    if  not cycle_index :\n",
    "        return None\n",
    "\n",
    "    min_cycle_index = 0\n",
    "\n",
    "    # Update min_cycle_index according to cycle_index and n\n",
    "    if cycle_index > n:\n",
    "        min_cycle_index = cycle_index - n\n",
    "\n",
    "    # Calculate the minimum value for random parent continuous id\n",
    "    min_rand_value = continuous_min_id_per_cycle[cycles[min_cycle_index]]\n",
    "\n",
    "    # Calculate the maximum value for random parent continuous id\n",
    "    max_rand_vale = continuous_min_id_per_cycle[cycles[cycle_index]] - 1\n",
    "\n",
    "    # Get random value of parent_continuous_id (from min_rand_value to max_rand_vale)\n",
    "    parent_continuous_id = random.randint(min_rand_value, max_rand_vale)\n",
    "\n",
    "    return parent_continuous_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23946054",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o100.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 17) (DESKTOP-2H598RF.mshome.net executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=3, The system cannot find the path specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: CreateProcess error=3, The system cannot find the path specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:444)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:139)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 49 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=3, The system cannot find the path specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=3, The system cannot find the path specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:444)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:139)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 49 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 10\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Create new column Parent Continuous ID using the assert_parent_bee_id_udf function and Cycle column\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# cache() caches the specified data frame in the memory of your cluster's workers\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# If executing multiple actions on the same data frame then cache it\u001B[39;00m\n\u001B[0;32m      7\u001B[0m df_cleaned \u001B[38;5;241m=\u001B[39m df_cleaned\u001B[38;5;241m.\u001B[39mwithColumn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParent Continuous ID\u001B[39m\u001B[38;5;124m\"\u001B[39m, assert_parent_bee_id_udf(col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCycle\u001B[39m\u001B[38;5;124m'\u001B[39m)))\\\n\u001B[0;32m      8\u001B[0m                         \u001B[38;5;241m.\u001B[39mcache()\n\u001B[1;32m---> 10\u001B[0m \u001B[43mdf_cleaned\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\dev\\Envs\\data\\lib\\site-packages\\pyspark\\sql\\dataframe.py:606\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[1;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[0;32m    603\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParameter \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m must be a bool\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    605\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[1;32m--> 606\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\dev\\Envs\\data\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[1;32m~\\dev\\Envs\\data\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    189\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 190\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[0;32m    191\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    192\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[1;32m~\\dev\\Envs\\data\\lib\\site-packages\\py4j\\protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o100.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 17) (DESKTOP-2H598RF.mshome.net executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=3, The system cannot find the path specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: CreateProcess error=3, The system cannot find the path specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:444)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:139)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 49 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=3, The system cannot find the path specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=3, The system cannot find the path specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:444)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:139)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 49 more\r\n"
     ]
    }
   ],
   "source": [
    "# Convert assert_parent_bee_id(cycle) to user defined function\n",
    "assert_parent_bee_id_udf = udf(lambda z: assert_parent_bee_id(z))\n",
    "\n",
    "# Create new column Parent Continuous ID using the assert_parent_bee_id_udf function and Cycle column\n",
    "# cache() caches the specified data frame in the memory of your cluster's workers\n",
    "# If executing multiple actions on the same data frame then cache it\n",
    "df_cleaned = df_cleaned.withColumn(\"Parent Continuous ID\", assert_parent_bee_id_udf(col('Cycle')))\\\n",
    "                        .cache()\n",
    "\n",
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create forest start"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a DataFrame of parent bees\n",
    "df_parent_bees = df_cleaned.select('Bee ID', 'Continuous ID')\n",
    "df_parent_bees = df_parent_bees.withColumnRenamed(\"Bee ID\",\"Parent Bee ID\")\n",
    "df_parent_bees = df_parent_bees.withColumnRenamed(\"Continuous ID\",\"Temp Continuous ID\")\n",
    "df_parent_bees= df_parent_bees.withColumn(\"Temp Continuous ID\",col(\"Temp Continuous ID\")\\\n",
    "                                  .cast(StringType()))\n",
    "\n",
    "df_parent_bees.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add the parent bee id to each bee id with the use of join\n",
    "df_beeId_parent_beeId= df_cleaned.join(df_parent_bees, df_cleaned['Parent Continuous ID'] == df_parent_bees['Temp Continuous ID'],'left')\n",
    "df_beeId_parent_beeId.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a forest\n",
    "def create_forest(df_childID_parentID):\n",
    "    tree =Tree()\n",
    "    tree.create_node('God', 'God')\n",
    "\n",
    "    # Add all the bees for cycle 0 to god\n",
    "    df_cleaned_0_cycle = df_childID_parentID.filter(col('Cycle') == 0)\n",
    "    for row in df_cleaned_0_cycle.collect():\n",
    "        tree.create_node(row['Bee ID'], row['Bee ID']+'-'+str(row['Continuous ID']), 'God')\n",
    "\n",
    "    # get all bees in cycle c\n",
    "    for cycle in cycles[1:]:\n",
    "        df_cleaned_cycle = df_childID_parentID.filter(col('Cycle') == cycle)\n",
    "        # Add bee from cycle c to tree\n",
    "        for row in df_cleaned_cycle.collect():\n",
    "            tree.create_node(row['Bee ID'], row['Bee ID']+'-'+str(row['Continuous ID']), row['Parent Bee ID']+'-'+row['Parent Continuous ID'])\n",
    "\n",
    "    return tree\n",
    "\n",
    "forest = create_forest(df_beeId_parent_beeId)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forest = create_forest(df_beeId_parent_beeId)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_name = 'forest.txt'\n",
    "forest.save2file(conf[APP]['data_save']+file_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create forest end"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Choose bee root for the tree\n",
    "bee_ancestor_id = '31_1711'\n",
    "bee_ancestors = df_cleaned.filter(df_cleaned['Bee ID'] == bee_ancestor_id).collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a tree using the idea of bfs algorithm\n",
    "# Slow version for alot of nodes\n",
    "# Works fast for a subtree (only one or few bees as roots)\n",
    "def create_tree_bfs(tree_nodes, df):\n",
    "\n",
    "    tree = Tree()\n",
    "\n",
    "    # Add first node/s(root/s) to tree\n",
    "    if len(tree_nodes) == 1:\n",
    "        tree.create_node(tree_nodes[0]['Bee ID'], tree_nodes[0]['Bee ID']+'-'+str(tree_nodes[0]['Continuous ID']))\n",
    "\n",
    "    else:\n",
    "        tree.create_node('God', 'God')\n",
    "        for root in tree_nodes:\n",
    "            tree.create_node(root['Bee ID'], root['Bee ID']+'-'+str(root['Continuous ID']), 'God')\n",
    "\n",
    "    # While tree_nodes not empty pop first value (parent) search for its kids append them to tree_nodes and add them to the tree\n",
    "    while tree_nodes:\n",
    "        # Get first value in tree_nodes\n",
    "        parent = tree_nodes.pop()\n",
    "\n",
    "        #find all rows in df_cleaned which their value in Parent Continuous ID column equals parent's Continuous ID\n",
    "        children = df.filter(col('Parent Continuous ID') == parent['Continuous ID'])\n",
    "\n",
    "        # for each kid in kids append to tree_nodes and add it to tree\n",
    "        for child in children.collect():\n",
    "                tree_nodes.append(child)\n",
    "                tree.create_node(child['Bee ID'], child['Bee ID']+'-'+str(child['Continuous ID']), parent['Bee ID']+'-'+str(parent['Continuous ID']) )\n",
    "\n",
    "    return tree"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create tree\n",
    "tree = create_tree_bfs(bee_ancestors,df_cleaned)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tree.size()\n",
    "tree.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a DataFrame with the best bee per cycle\n",
    "w = Window.partitionBy('Cycle')\n",
    "df_best_bee = df_cleaned.withColumn('minDaughtersEfficiencyScore', min('DaughtersEfficiencyScore').over(w))\\\n",
    "    .where(col('DaughtersEfficiencyScore') == col('minDaughtersEfficiencyScore'))\\\n",
    "    .drop('minDaughtersEfficiencyScore')\n",
    "\n",
    "df_best_bee.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the trees of all best bees using the function create_tree_bfs (option 1)\n",
    "best_bee_trees = {}\n",
    "\n",
    "for bee in df_best_bee.collect():\n",
    "    row = df_cleaned.filter(col('Bee ID')==bee['Bee ID']).collect()\n",
    "    best_bee_trees[bee['Bee ID']] = create_tree_bfs(row, df_cleaned)\n",
    "\n",
    "#best_bee_trees['<bee id from best bees>'].show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the trees of all best bees using the subtree of the forest(option 2)\n",
    "best_bee_trees = {}\n",
    "\n",
    "for bee in df_best_bee.collect():\n",
    "    best_bee_trees[bee['Bee ID']] = forest.subtree(bee['Bee ID']+'-'+str(bee['Continuous ID']))\n",
    "\n",
    "#best_bee_trees['<bee id from best bees>'].show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a tree of only the ancestors of the node\n",
    "def bee_ancestor_tree_1(node, tree):\n",
    "    # Get the depth of the node int the tree\n",
    "    depth_of_node = tree.depth(node['Bee ID']+'-'+str(node['Continuous ID']))\n",
    "    child = node['Bee ID']+'-'+str(node['Continuous ID'])\n",
    "    ancestors = [(node['Bee ID'],node['Bee ID']+'-'+str(node['Continuous ID'] ))]\n",
    "    # Add the ancestors of the node to the list\n",
    "    for i in range(depth_of_node):\n",
    "        parent = tree.parent(child)\n",
    "        ancestors.append((parent.tag,parent.identifier))\n",
    "        child = parent.identifier\n",
    "\n",
    "    # Reverse the ancestor list and add the oldest ancestor to the tree\n",
    "    tree_ancestors = Tree()\n",
    "    ancestors = ancestors[::-1]\n",
    "    oldest_ancestor = ancestors[0]\n",
    "    tree_ancestors.create_node(oldest_ancestor[0],oldest_ancestor[1])\n",
    "\n",
    "    # Add the ancestors of the node to the tree\n",
    "    for ancestor in ancestors[1:]:\n",
    "        tree_ancestors.create_node(ancestor[0],ancestor[1],oldest_ancestor[1])\n",
    "        oldest_ancestor = ancestor\n",
    "\n",
    "    return  tree_ancestors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the trees of all best bees ancestors using the function bee_ancestor_tree (option 1)\n",
    "best_bee_ancestors_trees = {}\n",
    "\n",
    "for bee in df_best_bee.collect():\n",
    "    best_bee_ancestors_trees[bee['Bee ID']] = bee_ancestor_tree_1(bee, forest)\n",
    "\n",
    "#best_bee_ancestors_trees['<bee id from best bees>'].show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bee_ancestor_tree_2(row, df_childID_parentID):\n",
    "    ancestors = [(row['Bee ID'], row['Bee ID']+'-'+str(row['Continuous ID']))]\n",
    "    ancestor = df_beeId_parent_beeId.filter(col('Continuous ID')==row['Parent Continuous ID']).collect()\n",
    "    tree_ancestors = Tree()\n",
    "\n",
    "    # If no ancestors return the tree with one node\n",
    "    if not ancestor:\n",
    "        tree_ancestors.create_node((row['Bee ID'], row['Bee ID']+'-'+str(row['Continuous ID'])))\n",
    "        return tree_ancestors\n",
    "\n",
    "    ancestor = ancestor[0]\n",
    "\n",
    "    while ancestor['Parent Bee ID']:\n",
    "        ancestors.append((ancestor['Bee ID'], ancestor['Bee ID']+'-'+str(ancestor['Continuous ID'])))\n",
    "        ancestor = df_beeId_parent_beeId.filter(col('Continuous ID')==ancestor['Parent Continuous ID']).collect()[0]\n",
    "\n",
    "    ancestors.append((ancestor['Bee ID'], ancestor['Bee ID']+'-'+str(ancestor['Continuous ID'])))\n",
    "\n",
    "    # Reverse the ancestor list and add the oldest ancestor to the tree\n",
    "    ancestors = ancestors[::-1]\n",
    "    oldest_ancestor = ancestors[0]\n",
    "    tree_ancestors.create_node(oldest_ancestor[0],oldest_ancestor[1])\n",
    "\n",
    "    # Add the ancestors of the node to the tree\n",
    "    for ancestor in ancestors[1:]:\n",
    "        tree_ancestors.create_node(ancestor[0],ancestor[1],oldest_ancestor[1])\n",
    "        oldest_ancestor = ancestor\n",
    "\n",
    "    return  tree_ancestors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the trees of all best bees ancestors using the function bee_ancestor_tree (option 1)\n",
    "best_bee_ancestors_trees = {}\n",
    "i=0\n",
    "for bee in df_best_bee.collect():\n",
    "    if not i%10:\n",
    "        print(i)\n",
    "    best_bee_ancestors_trees[bee['Bee ID']] = bee_ancestor_tree_2(bee, df_beeId_parent_beeId)\n",
    "    i+=1\n",
    "\n",
    "#best_bee_ancestors_trees['<bee id from best bees>'].show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
