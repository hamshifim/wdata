{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# Bee Hive data https://drive.google.com/file/d/142IBcs6OyQiJxO7owPfkEBFbkrudnh0g/view?usp=sharing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "APP = 'BeeHive'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "#! '../../package_py.bash'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,Row\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType\n",
    "from pyspark.sql.functions import split, row_number, udf, col, min, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.shell import sqlContext\n",
    "\n",
    "from pep_data.project import quick_conf\n",
    "from src.pep_data.spark.util import base_df\n",
    "\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(APP).getOrCreate()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# Get app configuration from project.conf file\n",
    "conf = quick_conf()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+\n",
      "|Bee ID|DaughtersEfficiencyScore|Father SIZE|Father TYPE|           X|          Y|          Z|\n",
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+\n",
      "|   0_0|           -0.1647494899|          5|     107027|    5.345708|25.54008605|24.79858692|\n",
      "|   0_9|          -0.09111780618|          8|      35473|  3.61617713|15.39993678|14.11150683|\n",
      "|  0_16|           -0.2783737487|          9|      72732| 6.487132473|27.96111467|23.50405554|\n",
      "|  0_76|           0.01988315069|          6|      49069| 8.285176906|21.88111447|10.80561155|\n",
      "|  0_35|           -0.0758420403|          5|        187|        -1.0|       13.0|       17.0|\n",
      "|  0_17|           -0.3362110457|          5|      74276|        -0.6|       12.0|       16.0|\n",
      "|  0_49|           -0.1311013747|         10|     108253| 4.123056465|16.28346035| 12.7359525|\n",
      "|  0_11|           -0.1881197044|          5|       8907|         2.4|       14.9|       12.7|\n",
      "|  0_53|             -1.07835423|          5|      56276|  0.48934967|24.60089192|24.29571403|\n",
      "|  0_58|           -0.1457884704|          5|       8639|        4.75|      15.75|      14.25|\n",
      "|  0_29|           -0.3677328995|          9|      16678| 5.347068171|24.86318585| 13.1676809|\n",
      "|  0_36|            -0.276260889|          7|      71010|        0.35|       15.0|      16.65|\n",
      "|   0_1|           -0.0337883593|         13|      23028|        0.85|      23.85|      22.25|\n",
      "|  0_60|           -0.7111252834|          7|      73856|0.6689851446|25.63599441|24.23511828|\n",
      "|   0_3|          -0.04991202534|         14|        782|0.8929233119|18.07738731| 16.2277547|\n",
      "|   0_4|           -0.2344739595|         12|      45682|  3.45375924|27.39807645|17.41006477|\n",
      "|  0_54|           -0.3550285031|          5|      35576|        -0.6|       22.9|       22.4|\n",
      "|  0_41|          -0.07477852786|          9|      88641|         1.5|       24.5|       21.6|\n",
      "|   0_5|         -0.004817857963|         14|        191| 3.198876948|24.93546125|15.27157968|\n",
      "|  0_48|            -0.237260917|          6|       1632| 9.395030718|14.13030347| 10.2961167|\n",
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = base_df(spark, conf, APP)\n",
    "\n",
    "df.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+-----+--------+\n",
      "|Bee ID|DaughtersEfficiencyScore|Father SIZE|Father TYPE|           X|          Y|          Z|Cycle|Cycle ID|\n",
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+-----+--------+\n",
      "|   0_0|           -0.1647494899|          5|     107027|    5.345708|25.54008605|24.79858692|    0|       0|\n",
      "|   0_9|          -0.09111780618|          8|      35473|  3.61617713|15.39993678|14.11150683|    0|       9|\n",
      "|  0_16|           -0.2783737487|          9|      72732| 6.487132473|27.96111467|23.50405554|    0|      16|\n",
      "|  0_76|           0.01988315069|          6|      49069| 8.285176906|21.88111447|10.80561155|    0|      76|\n",
      "|  0_35|           -0.0758420403|          5|        187|        -1.0|       13.0|       17.0|    0|      35|\n",
      "|  0_17|           -0.3362110457|          5|      74276|        -0.6|       12.0|       16.0|    0|      17|\n",
      "|  0_49|           -0.1311013747|         10|     108253| 4.123056465|16.28346035| 12.7359525|    0|      49|\n",
      "|  0_11|           -0.1881197044|          5|       8907|         2.4|       14.9|       12.7|    0|      11|\n",
      "|  0_53|             -1.07835423|          5|      56276|  0.48934967|24.60089192|24.29571403|    0|      53|\n",
      "|  0_58|           -0.1457884704|          5|       8639|        4.75|      15.75|      14.25|    0|      58|\n",
      "|  0_29|           -0.3677328995|          9|      16678| 5.347068171|24.86318585| 13.1676809|    0|      29|\n",
      "|  0_36|            -0.276260889|          7|      71010|        0.35|       15.0|      16.65|    0|      36|\n",
      "|   0_1|           -0.0337883593|         13|      23028|        0.85|      23.85|      22.25|    0|       1|\n",
      "|  0_60|           -0.7111252834|          7|      73856|0.6689851446|25.63599441|24.23511828|    0|      60|\n",
      "|   0_3|          -0.04991202534|         14|        782|0.8929233119|18.07738731| 16.2277547|    0|       3|\n",
      "|   0_4|           -0.2344739595|         12|      45682|  3.45375924|27.39807645|17.41006477|    0|       4|\n",
      "|  0_54|           -0.3550285031|          5|      35576|        -0.6|       22.9|       22.4|    0|      54|\n",
      "|  0_41|          -0.07477852786|          9|      88641|         1.5|       24.5|       21.6|    0|      41|\n",
      "|   0_5|         -0.004817857963|         14|        191| 3.198876948|24.93546125|15.27157968|    0|       5|\n",
      "|  0_48|            -0.237260917|          6|       1632| 9.395030718|14.13030347| 10.2961167|    0|      48|\n",
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split Bee_ID column to Cycle(Bee value) and Cycle ID(ID value) columns\n",
    "df_cleaned = df.withColumn('Cycle', split(col('Bee ID'), '_')\\\n",
    "                           .getItem(0))\\\n",
    "               .withColumn('Cycle ID', split(col('Bee ID'), '_')\\\n",
    "                           .getItem(1))\n",
    "\n",
    "# Change the type of value in Cycle column from string to integer\n",
    "df_cleaned= df_cleaned.withColumn(\"Cycle\",col(\"Cycle\")\\\n",
    "                                  .cast(IntegerType()))\n",
    "df_cleaned.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+-----+--------+-------------+\n",
      "|Bee ID|DaughtersEfficiencyScore|Father SIZE|Father TYPE|           X|          Y|          Z|Cycle|Cycle ID|Continuous ID|\n",
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+-----+--------+-------------+\n",
      "|   0_0|           -0.1647494899|          5|     107027|    5.345708|25.54008605|24.79858692|    0|       0|            1|\n",
      "|   0_9|          -0.09111780618|          8|      35473|  3.61617713|15.39993678|14.11150683|    0|       9|            2|\n",
      "|  0_16|           -0.2783737487|          9|      72732| 6.487132473|27.96111467|23.50405554|    0|      16|            3|\n",
      "|  0_76|           0.01988315069|          6|      49069| 8.285176906|21.88111447|10.80561155|    0|      76|            4|\n",
      "|  0_35|           -0.0758420403|          5|        187|        -1.0|       13.0|       17.0|    0|      35|            5|\n",
      "|  0_17|           -0.3362110457|          5|      74276|        -0.6|       12.0|       16.0|    0|      17|            6|\n",
      "|  0_49|           -0.1311013747|         10|     108253| 4.123056465|16.28346035| 12.7359525|    0|      49|            7|\n",
      "|  0_11|           -0.1881197044|          5|       8907|         2.4|       14.9|       12.7|    0|      11|            8|\n",
      "|  0_53|             -1.07835423|          5|      56276|  0.48934967|24.60089192|24.29571403|    0|      53|            9|\n",
      "|  0_58|           -0.1457884704|          5|       8639|        4.75|      15.75|      14.25|    0|      58|           10|\n",
      "|  0_29|           -0.3677328995|          9|      16678| 5.347068171|24.86318585| 13.1676809|    0|      29|           11|\n",
      "|  0_36|            -0.276260889|          7|      71010|        0.35|       15.0|      16.65|    0|      36|           12|\n",
      "|   0_1|           -0.0337883593|         13|      23028|        0.85|      23.85|      22.25|    0|       1|           13|\n",
      "|  0_60|           -0.7111252834|          7|      73856|0.6689851446|25.63599441|24.23511828|    0|      60|           14|\n",
      "|   0_3|          -0.04991202534|         14|        782|0.8929233119|18.07738731| 16.2277547|    0|       3|           15|\n",
      "|   0_4|           -0.2344739595|         12|      45682|  3.45375924|27.39807645|17.41006477|    0|       4|           16|\n",
      "|  0_54|           -0.3550285031|          5|      35576|        -0.6|       22.9|       22.4|    0|      54|           17|\n",
      "|  0_41|          -0.07477852786|          9|      88641|         1.5|       24.5|       21.6|    0|      41|           18|\n",
      "|   0_5|         -0.004817857963|         14|        191| 3.198876948|24.93546125|15.27157968|    0|       5|           19|\n",
      "|  0_48|            -0.237260917|          6|       1632| 9.395030718|14.13030347| 10.2961167|    0|      48|           20|\n",
      "+------+------------------------+-----------+-----------+------------+-----------+-----------+-----+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the data frame by Cycle column and add row number for each row(new column with the name Continuous ID)\n",
    "w = Window().orderBy('Cycle')\n",
    "df_cleaned = df_cleaned.withColumn('Continuous ID', row_number()\\\n",
    "                                   .over(w))\n",
    "\n",
    "df_cleaned.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# Create a dictionary with key = cycle , value = minimum value of Continuous ID of cycle(key)\n",
    "continuous_min_id_per_cycle = {key : value for key, value  in df_cleaned.groupBy('Cycle').min('Continuous ID').collect()}\n",
    "\n",
    "# continuous_min_id_per_cycle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# Create sorted list of all distinct values of Cycle column\n",
    "cycles = sorted([i[0] for i in df_cleaned.select('Cycle').distinct().collect()])\n",
    "\n",
    "# cycles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "# Return parent continuous id according to cycle and n\n",
    "def assert_parent_bee_id(cycle):\n",
    "    n = 3\n",
    "\n",
    "    # Get index  of cycle in cycles list\n",
    "    cycle_index = cycles.index(cycle)\n",
    "\n",
    "    # Return parent bee id if cycle is 0\n",
    "    if  not cycle_index :\n",
    "        return None\n",
    "\n",
    "    min_cycle_index = 0\n",
    "\n",
    "    # Update min_cycle_index according to cycle_index and n\n",
    "    if cycle_index > n:\n",
    "        min_cycle_index = cycle_index - n\n",
    "\n",
    "    # Calculate the minimum value for random parent continuous id\n",
    "    min_rand_value = continuous_min_id_per_cycle[cycles[min_cycle_index]]\n",
    "\n",
    "    # Calculate the maximum value for random parent continuous id\n",
    "    max_rand_vale = continuous_min_id_per_cycle[cycles[cycle_index]] - 1\n",
    "\n",
    "    # Get random value of parent_continuous_id (from min_rand_value to max_rand_vale)\n",
    "    parent_continuous_id = random.randint(min_rand_value, max_rand_vale)\n",
    "\n",
    "    return parent_continuous_id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# TODO group by some columns to create partitions\n",
    "\n",
    "# Convert assert_parent_bee_id(cycle) to user defined function\n",
    "assert_parent_bee_id_udf = udf(lambda z: assert_parent_bee_id(z))\n",
    "\n",
    "# Create new column Parent Continuous ID using the assert_parent_bee_id_udf function and Cycle column\n",
    "# cache() caches the specified data frame in the memory of your cluster's workers\n",
    "# If executing multiple actions on the same data frame then cache it\n",
    "df_cleaned = df_cleaned.withColumn(\"Parent Continuous ID\", assert_parent_bee_id_udf(col('Cycle')))\\\n",
    "                        .cache()\n",
    "\n",
    "#df_cleaned.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "# Create a DataFrame of parent bees\n",
    "df_parent_bees = df_cleaned.select('Bee ID', 'Continuous ID')\n",
    "df_parent_bees = df_parent_bees.withColumnRenamed(\"Bee ID\",\"Parent Bee ID\")\n",
    "df_parent_bees = df_parent_bees.withColumnRenamed(\"Continuous ID\",\"Temp Continuous ID\")\n",
    "df_parent_bees= df_parent_bees.withColumn(\"Temp Continuous ID\",col(\"Temp Continuous ID\")\\\n",
    "                                  .cast(StringType()))\n",
    "\n",
    "#df_parent_bees.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# Add the parent bee id to each bee id with the use of join\n",
    "df_beeId_parent_beeId= df_cleaned.join(df_parent_bees, df_cleaned['Parent Continuous ID'] == df_parent_bees['Temp Continuous ID'],'left').drop('Temp Continuous ID')\n",
    "#df_beeId_parent_beeId.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# Split Bee_ID column to Cycle(Bee value) and Cycle ID(ID value) columns\n",
    "df_beeId_parent_beeId = df_beeId_parent_beeId.withColumn('Parent Cycle', split(col('Parent Bee ID'), '_')\\\n",
    "                           .getItem(0))\n",
    "\n",
    "\n",
    "# Change the type of value in Cycle column from string to integer\n",
    "df_beeId_parent_beeId= df_beeId_parent_beeId.withColumn(\"Parent Cycle\",col(\"Parent Cycle\")\\\n",
    "                                  .cast(IntegerType()))\n",
    "# df_beeId_parent_beeId.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# Add oldest Ancestor column\n",
    "df_beeId_parent_beeId = df_beeId_parent_beeId.withColumn('Oldest Ancestor',\n",
    "                                                         when(col('Cycle')==0,None).\n",
    "                                                         otherwise(''))\n",
    "# df_beeId_parent_beeId.toPandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# Add Ancestors column\n",
    "df_beeId_parent_beeId = df_beeId_parent_beeId.withColumn('Ancestors',\n",
    "                                                         when(col('Cycle')==0,None).\n",
    "                                                         otherwise(''))\n",
    "# df_beeId_parent_beeId.toPandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "# Assert the values of Ancestors and old Oldest Ancestor colums for the children of cycle 0\n",
    "def assert_ancestors_and_oldestancestors_columns_children(row, list_cycle_0):\n",
    "    row_dict = row.asDict()\n",
    "\n",
    "    for bee in list_cycle_0:\n",
    "        if row_dict['Parent Continuous ID'] == str(bee['Continuous ID']):\n",
    "            row_dict['Oldest Ancestor'] = bee['Bee ID']+'-'+str(bee['Continuous ID'])\n",
    "            row_dict['Ancestors'] = bee['Bee ID']+'-'+str(bee['Continuous ID'])\n",
    "            break\n",
    "\n",
    "    newrow = Row(**row_dict)\n",
    "\n",
    "    return newrow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o849.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 45.0 failed 1 times, most recent failure: Lost task 0.0 in stage 45.0 (TID 55) (DESKTOP-2H598RF.mshome.net executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=3, The system cannot find the path specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: CreateProcess error=3, The system cannot find the path specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:444)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:139)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 49 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:431)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:137)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:191)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=3, The system cannot find the path specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\t... 3 more\r\nCaused by: java.io.IOException: CreateProcess error=3, The system cannot find the path specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:444)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:139)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 49 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[72], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Data frame with only cycle 0 children\u001B[39;00m\n\u001B[0;32m      4\u001B[0m df_parent_cycle \u001B[38;5;241m=\u001B[39m df_beeId_parent_beeId\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mParent Cycle\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m----> 6\u001B[0m list_cycle_0 \u001B[38;5;241m=\u001B[39m \u001B[43mdf_enriched\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Convert the data frame to rdd and assert the values of Oldest Ancestor and Ancestors columns\u001B[39;00m\n\u001B[0;32m      8\u001B[0m df_parent_cycle_rdd \u001B[38;5;241m=\u001B[39m df_parent_cycle\u001B[38;5;241m.\u001B[39mrdd\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m row: assert_ancestors_and_oldestancestors_columns_children(row,list_cycle_0))\u001B[38;5;241m.\u001B[39mcache()\n",
      "File \u001B[1;32m~\\dev\\Envs\\data\\lib\\site-packages\\pyspark\\sql\\dataframe.py:817\u001B[0m, in \u001B[0;36mDataFrame.collect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    807\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001B[39;00m\n\u001B[0;32m    808\u001B[0m \n\u001B[0;32m    809\u001B[0m \u001B[38;5;124;03m.. versionadded:: 1.3.0\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    814\u001B[0m \u001B[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001B[39;00m\n\u001B[0;32m    815\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    816\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SCCallSiteSync(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc):\n\u001B[1;32m--> 817\u001B[0m     sock_info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollectToPython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    818\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001B[1;32m~\\dev\\Envs\\data\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[1;32m~\\dev\\Envs\\data\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    189\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 190\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[0;32m    191\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    192\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[1;32m~\\dev\\Envs\\data\\lib\\site-packages\\py4j\\protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o849.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 45.0 failed 1 times, most recent failure: Lost task 0.0 in stage 45.0 (TID 55) (DESKTOP-2H598RF.mshome.net executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=3, The system cannot find the path specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: CreateProcess error=3, The system cannot find the path specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:444)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:139)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 49 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:431)\r\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:137)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:191)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=3, The system cannot find the path specified\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\t... 3 more\r\nCaused by: java.io.IOException: CreateProcess error=3, The system cannot find the path specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(ProcessImpl.java:444)\r\n\tat java.lang.ProcessImpl.start(ProcessImpl.java:139)\r\n\tat java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)\r\n\t... 49 more\r\n"
     ]
    }
   ],
   "source": [
    "# Data frame with additional columns with only cycle 0\n",
    "df_enriched = df_beeId_parent_beeId.filter(col('Cycle')==0)\n",
    "# Data frame with only cycle 0 children\n",
    "df_parent_cycle = df_beeId_parent_beeId.filter(col('Parent Cycle')==0)\n",
    "\n",
    "list_cycle_0 = df_enriched.collect()\n",
    "# Convert the data frame to rdd and assert the values of Oldest Ancestor and Ancestors columns\n",
    "df_parent_cycle_rdd = df_parent_cycle.rdd.map(lambda row: assert_ancestors_and_oldestancestors_columns_children(row,list_cycle_0)).cache()\n",
    "df_parent_cycle = sqlContext.createDataFrame(df_parent_cycle_rdd,schema= df_enriched.schema)\n",
    "# Union the initial enriched data frame with the result\n",
    "df_enriched = df_enriched.union(df_parent_cycle)\n",
    "df_enriched.toPandas()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assert the values of Ancestors and old Oldest Ancestor colums for the children of cycle n (all cycles except cycle 0)\n",
    "def assert_ancestors_and_oldestancestors_columns_grandchildren(row, list_n_cycles):\n",
    "\n",
    "    row_dict = row.asDict()\n",
    "\n",
    "    for bee in list_n_cycles:\n",
    "        if row_dict['Parent Continuous ID'] == str(bee['Continuous ID']):\n",
    "            row_dict['Oldest Ancestor'] = bee['Oldest Ancestor']\n",
    "            row_dict['Ancestors'] = bee['Ancestors'] + '->' +row_dict['Parent Bee ID']+'-'+str(row_dict['Parent Continuous ID'])\n",
    "            break\n",
    "\n",
    "    newrow = Row(**row_dict)\n",
    "\n",
    "    return newrow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For all the cycle except the first, get df of children pf cycle c, get list of its parents\n",
    "# convert df to rdd and use the assert_ancestors_and_oldestancestors_columns_grandchildren to assert\n",
    "# values to the new columns, union the result with the existing enriched df\n",
    "n=3\n",
    "for cycle in cycles[1:-1:]:\n",
    "    df_cycle = df_beeId_parent_beeId.filter(col('Parent Cycle')==cycle)\n",
    "    # list_n_cycles = df_enriched.select('Cycle', 'Continuous ID', 'Oldest Ancestor','Ancestors' )\\\n",
    "    #                             .filter(col('Cycle')==cycle).collect()\n",
    "    list_n_cycles = df_enriched.select('Cycle', 'Continuous ID', 'Oldest Ancestor','Ancestors' )\\\n",
    "                                .filter((col('Parent Cycle')<cycle) & (col('Parent Cycle')>=cycle-n)).collect()\n",
    "\n",
    "    # Convert the data frame to rdd and assert the values of Oldest Ancestor and Ancestors columns\n",
    "    df_cycle_rdd = df_cycle.rdd.map(lambda row: assert_ancestors_and_oldestancestors_columns_grandchildren(row,list_n_cycles)).cache()\n",
    "    df_cycle = sqlContext.createDataFrame(df_cycle_rdd,schema= df_enriched.schema)\n",
    "    # Union the enriched data frame with the result\n",
    "    df_enriched = df_enriched.union(df_cycle)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save data frame to csv file\n",
    "file_name = '2.csv'\n",
    "save_path = conf[APP]['data_save']\n",
    "df_enriched.coalesce(1).write.option('header',True).\\\n",
    "csv(save_path+file_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, collect_set\n",
    "df_cycle_beeID_list = df_beeId_parent_beeId.groupBy('Cycle').agg(collect_list('Bee ID'))\n",
    "df_cycle_beeID_list.toPandas()\n",
    "#df_cycle_beeID_list.foreach()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
