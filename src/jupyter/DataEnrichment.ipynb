{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bee Hive data https://drive.google.com/file/d/142IBcs6OyQiJxO7owPfkEBFbkrudnh0g/view?usp=sharing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "APP = 'BeeHive'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "! '../../package_py.bash'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,Row\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType\n",
    "from pyspark.sql.functions import split, row_number, udf, col, min, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.shell import sqlContext\n",
    "\n",
    "from pep_data.project import quick_conf\n",
    "from pep_data.spark.util import field_to_struct\n",
    "\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(APP).getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get app configuration from project.conf file\n",
    "conf = quick_conf()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create schema for the data\n",
    "cols_name = conf[APP]['cols_name']\n",
    "cols_double = conf[APP]['cols_double']\n",
    "cols_integer = conf[APP]['cols_integer']\n",
    "\n",
    "# Create all the fields\n",
    "fields = [field_to_struct(header, doubles=cols_double, integers=cols_integer) for header in cols_name]\n",
    "\n",
    "# Create the schema from th e fields\n",
    "schema = StructType(fields)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the data from the csv using the schema\n",
    "data_path = conf[APP]['data_path']\n",
    "df = spark.read.schema(schema).csv(data_path)\n",
    "\n",
    "#df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove unnecessary columns(the columns with the word remove in them)\n",
    "cols_to_keep = [x for x in df.columns if 'remove' not in x]\n",
    "df = df.select(*cols_to_keep)\n",
    "\n",
    "#df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split Bee_ID column to Cycle(Bee value) and Cycle ID(ID value) columns\n",
    "df_cleaned = df.withColumn('Cycle', split(col('Bee ID'), '_')\\\n",
    "                           .getItem(0))\\\n",
    "               .withColumn('Cycle ID', split(col('Bee ID'), '_')\\\n",
    "                           .getItem(1))\n",
    "\n",
    "# Change the type of value in Cycle column from string to integer\n",
    "df_cleaned= df_cleaned.withColumn(\"Cycle\",col(\"Cycle\")\\\n",
    "                                  .cast(IntegerType()))\n",
    "#df_cleaned.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sort the data frame by Cycle column and add row number for each row(new column with the name Continuous ID)\n",
    "w = Window().orderBy('Cycle')\n",
    "df_cleaned = df_cleaned.withColumn('Continuous ID', row_number()\\\n",
    "                                   .over(w))\n",
    "\n",
    "# df_cleaned.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a dictionary with key = cycle , value = minimum value of Continuous ID of cycle(key)\n",
    "continuous_min_id_per_cycle = {key : value for key, value  in df_cleaned.groupBy('Cycle').min('Continuous ID').collect()}\n",
    "\n",
    "# continuous_min_id_per_cycle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create sorted list of all distinct values of Cycle column\n",
    "cycles = sorted([i[0] for i in df_cleaned.select('Cycle').distinct().collect()])\n",
    "\n",
    "# cycles"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Return parent continuous id according to cycle and n\n",
    "def assert_parent_bee_id(cycle):\n",
    "    n = 3\n",
    "\n",
    "    # Get index  of cycle in cycles list\n",
    "    cycle_index = cycles.index(cycle)\n",
    "\n",
    "    # Return parent bee id if cycle is 0\n",
    "    if  not cycle_index :\n",
    "        return None\n",
    "\n",
    "    min_cycle_index = 0\n",
    "\n",
    "    # Update min_cycle_index according to cycle_index and n\n",
    "    if cycle_index > n:\n",
    "        min_cycle_index = cycle_index - n\n",
    "\n",
    "    # Calculate the minimum value for random parent continuous id\n",
    "    min_rand_value = continuous_min_id_per_cycle[cycles[min_cycle_index]]\n",
    "\n",
    "    # Calculate the maximum value for random parent continuous id\n",
    "    max_rand_vale = continuous_min_id_per_cycle[cycles[cycle_index]] - 1\n",
    "\n",
    "    # Get random value of parent_continuous_id (from min_rand_value to max_rand_vale)\n",
    "    parent_continuous_id = random.randint(min_rand_value, max_rand_vale)\n",
    "\n",
    "    return parent_continuous_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO group by some columns to create partitions\n",
    "\n",
    "# Convert assert_parent_bee_id(cycle) to user defined function\n",
    "assert_parent_bee_id_udf = udf(lambda z: assert_parent_bee_id(z))\n",
    "\n",
    "# Create new column Parent Continuous ID using the assert_parent_bee_id_udf function and Cycle column\n",
    "# cache() caches the specified data frame in the memory of your cluster's workers\n",
    "# If executing multiple actions on the same data frame then cache it\n",
    "df_cleaned = df_cleaned.withColumn(\"Parent Continuous ID\", assert_parent_bee_id_udf(col('Cycle')))\\\n",
    "                        .cache()\n",
    "\n",
    "# df_cleaned.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a DataFrame of parent bees\n",
    "df_parent_bees = df_cleaned.select('Bee ID', 'Continuous ID')\n",
    "df_parent_bees = df_parent_bees.withColumnRenamed(\"Bee ID\",\"Parent Bee ID\")\n",
    "df_parent_bees = df_parent_bees.withColumnRenamed(\"Continuous ID\",\"Temp Continuous ID\")\n",
    "df_parent_bees= df_parent_bees.withColumn(\"Temp Continuous ID\",col(\"Temp Continuous ID\")\\\n",
    "                                  .cast(StringType()))\n",
    "\n",
    "# df_parent_bees.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add the parent bee id to each bee id with the use of join\n",
    "df_beeId_parent_beeId= df_cleaned.join(df_parent_bees, df_cleaned['Parent Continuous ID'] == df_parent_bees['Temp Continuous ID'],'left').drop('Temp Continuous ID')\n",
    "# df_beeId_parent_beeId.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split Bee_ID column to Cycle(Bee value) and Cycle ID(ID value) columns\n",
    "df_beeId_parent_beeId = df_beeId_parent_beeId.withColumn('Parent Cycle', split(col('Parent Bee ID'), '_')\\\n",
    "                           .getItem(0))\n",
    "\n",
    "\n",
    "# Change the type of value in Cycle column from string to integer\n",
    "df_beeId_parent_beeId= df_beeId_parent_beeId.withColumn(\"Parent Cycle\",col(\"Parent Cycle\")\\\n",
    "                                  .cast(IntegerType()))\n",
    "# df_beeId_parent_beeId.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add oldest Ancestor column\n",
    "df_beeId_parent_beeId = df_beeId_parent_beeId.withColumn('Oldest Ancestor',\n",
    "                                                         when(col('Cycle')==0,None).\n",
    "                                                         otherwise(''))\n",
    "# df_beeId_parent_beeId.toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add Ancestors column\n",
    "df_beeId_parent_beeId = df_beeId_parent_beeId.withColumn('Ancestors',\n",
    "                                                         when(col('Cycle')==0,None).\n",
    "                                                         otherwise(''))\n",
    "# df_beeId_parent_beeId.toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assert the values of Ancestors and old Oldest Ancestor colums for the children of cycle 0\n",
    "def assert_ancestors_and_oldestancestors_columns_children(row, list_cycle_0):\n",
    "    row_dict = row.asDict()\n",
    "\n",
    "    for bee in list_cycle_0:\n",
    "        if row_dict['Parent Continuous ID'] == str(bee['Continuous ID']):\n",
    "            row_dict['Oldest Ancestor'] = bee['Bee ID']+'-'+str(bee['Continuous ID'])\n",
    "            row_dict['Ancestors'] = bee['Bee ID']+'-'+str(bee['Continuous ID'])\n",
    "            break\n",
    "\n",
    "    newrow = Row(**row_dict)\n",
    "\n",
    "    return newrow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data frame with additional columns with only cycle 0\n",
    "df_enriched = df_beeId_parent_beeId.filter(col('Cycle')==0)\n",
    "# Data frame with only cycle 0 children\n",
    "df_parent_cycle = df_beeId_parent_beeId.filter(col('Parent Cycle')==0)\n",
    "\n",
    "list_cycle_0 = df_enriched.collect()\n",
    "# Convert the data frame to rdd and assert the values of Oldest Ancestor and Ancestors columns\n",
    "df_parent_cycle_rdd = df_parent_cycle.rdd.map(lambda row: assert_ancestors_and_oldestancestors_columns_children(row,list_cycle_0)).cache()\n",
    "df_parent_cycle = sqlContext.createDataFrame(df_parent_cycle_rdd,schema= df_enriched.schema)\n",
    "# Union the initial enriched data frame with the result\n",
    "df_enriched = df_enriched.union(df_parent_cycle)\n",
    "df_enriched.toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assert the values of Ancestors and old Oldest Ancestor colums for the children of cycle n (all cycles except cycle 0)\n",
    "def assert_ancestors_and_oldestancestors_columns_grandchildren(row, list_n_cycles):\n",
    "\n",
    "    row_dict = row.asDict()\n",
    "\n",
    "    for bee in list_n_cycles:\n",
    "        if row_dict['Parent Continuous ID'] == str(bee['Continuous ID']):\n",
    "            row_dict['Oldest Ancestor'] = bee['Oldest Ancestor']\n",
    "            row_dict['Ancestors'] = bee['Ancestors'] + '->' +row_dict['Parent Bee ID']+'-'+str(row_dict['Parent Continuous ID'])\n",
    "            break\n",
    "\n",
    "    newrow = Row(**row_dict)\n",
    "\n",
    "    return newrow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For all the cycle except the first, get df of children pf cycle c, get list of its parents\n",
    "# convert df to rdd and use the assert_ancestors_and_oldestancestors_columns_grandchildren to assert\n",
    "# values to the new columns, union the result with the existing enriched df\n",
    "n=3\n",
    "for cycle in cycles[1:-1:]:\n",
    "    df_cycle = df_beeId_parent_beeId.filter(col('Parent Cycle')==cycle)\n",
    "    # list_n_cycles = df_enriched.select('Cycle', 'Continuous ID', 'Oldest Ancestor','Ancestors' )\\\n",
    "    #                             .filter(col('Cycle')==cycle).collect()\n",
    "    list_n_cycles = df_enriched.select('Cycle', 'Continuous ID', 'Oldest Ancestor','Ancestors' )\\\n",
    "                                .filter((col('Parent Cycle')<cycle) & (col('Parent Cycle')>=cycle-n)).collect()\n",
    "\n",
    "    # Convert the data frame to rdd and assert the values of Oldest Ancestor and Ancestors columns\n",
    "    df_cycle_rdd = df_cycle.rdd.map(lambda row: assert_ancestors_and_oldestancestors_columns_grandchildren(row,list_n_cycles)).cache()\n",
    "    df_cycle = sqlContext.createDataFrame(df_cycle_rdd,schema= df_enriched.schema)\n",
    "    # Union the enriched data frame with the result\n",
    "    df_enriched = df_enriched.union(df_cycle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save data frame to csv file\n",
    "file_name = '2.csv'\n",
    "save_path = conf[APP]['data_save']\n",
    "df_enriched.coalesce(1).write.option('header',True).\\\n",
    "csv(save_path+file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, collect_set\n",
    "df_cycle_beeID_list = df_beeId_parent_beeId.groupBy('Cycle').agg(collect_list('Bee ID'))\n",
    "df_cycle_beeID_list.toPandas()\n",
    "#df_cycle_beeID_list.foreach()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}